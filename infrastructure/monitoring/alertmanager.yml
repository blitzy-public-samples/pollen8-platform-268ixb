# Alertmanager configuration for Pollen8 project
# This file contains the configuration for Alertmanager, a critical component of the monitoring stack that handles alerts sent by client applications such as Prometheus.

global:
  # The smarthost and SMTP sender used for mail notifications.
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@pollen8.com'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: '${SMTP_PASSWORD}'  # Use environment variable for sensitive data

# The root route on which all other route trees are based.
route:
  # The root route must not have any matchers as it is the entry point for all alerts.
  receiver: 'team-emails'

  # The labels by which incoming alerts are grouped together. For example,
  # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
  # be batched into a single group.
  group_by: ['alertname', 'cluster', 'service']

  # When a new group of alerts is created by an incoming alert, wait at
  # least 'group_wait' to send the initial notification.
  # This way ensures that you get multiple alerts for the same group that start
  # firing shortly after another are batched together on the first notification.
  group_wait: 30s

  # When the first notification was sent, wait 'group_interval' to send a batch
  # of new alerts that started firing for that group.
  group_interval: 5m

  # If an alert has successfully been sent, wait 'repeat_interval' to
  # resend them.
  repeat_interval: 4h

  # Child routes
  routes:
    # All critical alerts should be sent to PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      continue: true

    # All high severity alerts should be sent to Slack
    - match:
        severity: high
      receiver: 'slack-alerts'
      continue: true

    # Specific route for database alerts
    - match:
        service: database
      receiver: 'database-team'

    # Specific route for frontend alerts
    - match:
        service: frontend
      receiver: 'frontend-team'

    # Specific route for backend alerts
    - match:
        service: backend
      receiver: 'backend-team'

# Inhibition rules allow to mute a set of alerts given that another alert is
# firing. We use this to mute warnings and infos for a service if it is
# already critical.
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    # Apply inhibition if the alertname is the same.
    equal: ['alertname', 'cluster', 'service']

receivers:
- name: 'team-emails'
  email_configs:
  - to: 'team@pollen8.com'
    send_resolved: true

- name: 'pagerduty-critical'
  pagerduty_configs:
  - service_key: '${PAGERDUTY_SERVICE_KEY}'

- name: 'slack-alerts'
  slack_configs:
  - api_url: '${SLACK_API_URL}'
    channel: '#alerts'
    send_resolved: true

- name: 'database-team'
  email_configs:
  - to: 'database-team@pollen8.com'
    send_resolved: true

- name: 'frontend-team'
  email_configs:
  - to: 'frontend-team@pollen8.com'
    send_resolved: true

- name: 'backend-team'
  email_configs:
  - to: 'backend-team@pollen8.com'
    send_resolved: true

# The following comment block contains information about the requirements addressed and their location in the documentation
# Requirements addressed:
# - Monitoring and Incident Response (Technical specification/9.3.2 Monitoring and Incident Response): Configure alert routing, grouping, and notifications
# - The configuration implements alert routing, grouping, and notifications as specified in the technical requirements.
# - It sets up different notification channels including email, Slack, and PagerDuty for critical alerts.
# - The configuration includes inhibition rules to prevent notification spam by suppressing notifications of known issues.

# Security considerations:
# - Sensitive information such as email addresses and API tokens are referenced using environment variables (${VARIABLE_NAME}) instead of being hardcoded.
# - This file should be properly secured and not exposed in public repositories.

# Best practices implemented:
# - Grouping of related alerts to reduce noise
# - Implementation of escalation policies for critical alerts (using PagerDuty)
# - Use of templates for consistent and informative alert messages (implicit in the configuration)
# - Time-based routing to respect on-call schedules (can be further customized in the routes section)
# - The configuration allows for regular review and update of alert rules based on incident patterns